{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パッケージのimport\n",
    "import numpy as np\n",
    "import random\n",
    "import math \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12)\n",
    "np.random.seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nembedding\\nposition encoding\\ndropout\\n\\n###1Hopping###\\nLayer Normalization\\nself_attention\\ndropout\\nLayer Normalization\\nFFN\\ndropout\\n\\nLayer Normalization\\nself_attention\\ndropout\\nLayer Normalization\\nFFN\\ndropout\\n############\\n\\nlinear\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "embedding\n",
    "position encoding\n",
    "dropout\n",
    "\n",
    "###1Hopping###\n",
    "Layer Normalization\n",
    "self_attention\n",
    "dropout\n",
    "Layer Normalization\n",
    "FFN\n",
    "dropout\n",
    "\n",
    "Layer Normalization\n",
    "self_attention\n",
    "dropout\n",
    "Layer Normalization\n",
    "FFN\n",
    "dropout\n",
    "############\n",
    "\n",
    "linear\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#埋め込み層\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, text_embedding_vectors):\n",
    "        super(Embedder, self).__init__()\n",
    "        \n",
    "        #更新はしない\n",
    "        self.embeddings = nn.Embedding.from_pretrained(\n",
    "            embeddings=text_embedding_vectors, freeze=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_vec = self.embeddings(x)\n",
    "\n",
    "        return x_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PositonalEncoding\n",
    "class PositionalEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=300, max_seq_len=140):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "\n",
    "        # GPUが使える場合はGPUへ送る、ここでは省略。実際に学習時には使用する\n",
    "        # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # pe = pe.to(device)\n",
    "\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos /\n",
    "                                          (10000 ** ((2 * (i + 1))/d_model)))\n",
    "\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "        self.pe.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        ret = math.sqrt(self.d_model)*x + self.pe\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_model, head_num, dropout_rate):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        d_model：出力層の次元(head_bumの倍数)\n",
    "        head_num：ヘッドの数\n",
    "        dropout_rate\n",
    "        \"\"\"\n",
    "        self.d_model = d_model\n",
    "        self.head_num = head_num\n",
    "        self.dropout_rate = dropout_rate\n",
    "    \n",
    "        #特徴量変換\n",
    "        self.q_linear = nn.Linear(d_model, d_model) \n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        #出力の全結合層\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        self.attention_dropout_layer = nn.Dropout(dropout_rate)   \n",
    "    \n",
    "    def forward(self, q, k, v, mask):\n",
    "        #key, query, valueを生成\n",
    "        q = self.q_linear(q) # [batch_size, max_seq_len, d_model]\n",
    "        k = self.q_linear(k) \n",
    "        v = self.q_linear(v)\n",
    "        \n",
    "        #head_numに分割\n",
    "        q = self._split_head(q) # [batch_size, head_num, max_seq_len, d_model/head_num]\n",
    "        k = self._split_head(k)\n",
    "        v = self._split_head(v)\n",
    "        \n",
    "        #queryとkeyの関連度の計算と、Scaled Dot-production\n",
    "        weights = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(self.d_model) # [batch_size, head_num, max_seq_len, max_seq_len]\n",
    "        \n",
    "        #maskをかける\n",
    "        #multi-headを使う場合のmask\n",
    "        #mask = mask.unsqueeze(1).unsqueeze(1)\n",
    "        mask = mask.unsqueeze(1)\n",
    "        weights = weights.masked_fill(mask==0, -1e9)# [batch_size, head_num, max_seq_len, max_seq_len]\n",
    "        \n",
    "        #AttentionWeightを計算\n",
    "        attention_weight = F.softmax(weights, dim=-1)# [batch_size, head_num, q_length, k_length]\n",
    "        \n",
    "        #AttentionWeightよりvalueから情報を引き出す\n",
    "        attention_output = torch.matmul(attention_weight, v)# [batch_size, head_num, q_length, d_model/head_num]\n",
    "        attention_output = self._combine_head(attention_output)\n",
    "        output = self.out(attention_output)\n",
    "        \n",
    "        \n",
    "        return output, attention_weight\n",
    "        \n",
    "    def _split_head(self, x):\n",
    "        \"\"\"\n",
    "        x.size:[batch_size, length, d_model]\n",
    "        \"\"\"\n",
    "        batch_size, length, d_model = x.size()\n",
    "        x = x.view(batch_size, length, self.head_num, self.d_model//self.head_num) #reshape\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "    #outputする前に分割したheadを戻す。\n",
    "    def _combine_head(self, x):\n",
    "        \"\"\"\n",
    "        x.size:[batch_size, head_num, length, d_model//head_num]\n",
    "        \"\"\"\n",
    "        batch_size, _, length, _  = x.size()\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        return x.reshape(batch_size, length, self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x size=[batch_size, length, d_model]\n",
    "        return size=[batch_size, length, d_model]\n",
    "        \"\"\"\n",
    "        x = self.linear_1(x)\n",
    "        x = self.dropout(F.relu(x))\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, head_num, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # LayerNormalization\n",
    "        self.norm_1 = nn.LayerNorm(d_model)\n",
    "        self.norm_2 = nn.LayerNorm(d_model)\n",
    "        # Attention\n",
    "        self.attn = MultiheadAttention(d_model, head_num, dropout)\n",
    "        # FFN\n",
    "        self.ff = FeedForward(d_model)\n",
    "        # Dropout\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # SelfAttention\n",
    "        x_normlized = self.norm_1(x)\n",
    "        output, normlized_weights = self.attn(\n",
    "            x_normlized, x_normlized, x_normlized, mask)\n",
    "        x2 = x + self.dropout_1(output)\n",
    "        # FFN\n",
    "        x_normlized2 = self.norm_2(x2)\n",
    "        output = x2 + self.dropout_2(self.ff(x_normlized2))\n",
    "\n",
    "        return output, normlized_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['その', '棚', 'に', 'ある', '赤い', 'りんご', 'は', 'とても', 'まずい']\n",
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "weights torch.Size([1, 5, 140, 140])\n",
      "weights tensor([[[[ 1.4868,  0.1113,  0.1579,  ...,  0.9178,  1.0648,  1.2089],\n",
      "          [ 0.1113,  1.2679,  0.3533,  ...,  0.0626,  0.0792,  0.0570],\n",
      "          [ 0.1579,  0.3533,  1.1271,  ...,  0.1796,  0.2157,  0.2079],\n",
      "          ...,\n",
      "          [ 0.9178,  0.0626,  0.1796,  ...,  1.2913,  1.3619,  1.3662],\n",
      "          [ 1.0648,  0.0792,  0.2157,  ...,  1.3619,  1.4970,  1.5662],\n",
      "          [ 1.2089,  0.0570,  0.2079,  ...,  1.3662,  1.5662,  1.7231]],\n",
      "\n",
      "         [[ 1.5218,  0.1483,  0.3181,  ...,  0.7013,  0.7579,  0.8161],\n",
      "          [ 0.1483,  1.3289,  0.0397,  ...,  0.1829,  0.1760,  0.1561],\n",
      "          [ 0.3181,  0.0397,  0.8664,  ...,  0.1796,  0.1427,  0.0914],\n",
      "          ...,\n",
      "          [ 0.7013,  0.1829,  0.1796,  ...,  1.0274,  1.0025,  0.9447],\n",
      "          [ 0.7579,  0.1760,  0.1427,  ...,  1.0025,  1.0297,  1.0251],\n",
      "          [ 0.8161,  0.1561,  0.0914,  ...,  0.9447,  1.0251,  1.0876]],\n",
      "\n",
      "         [[ 0.9396,  0.2082,  0.0258,  ...,  0.4010,  0.4120,  0.4148],\n",
      "          [ 0.2082,  0.9780,  0.3019,  ...,  0.0133, -0.0170, -0.0616],\n",
      "          [ 0.0258,  0.3019,  1.2403,  ...,  0.0046,  0.0045, -0.0253],\n",
      "          ...,\n",
      "          [ 0.4010,  0.0133,  0.0046,  ...,  0.9588,  0.9310,  0.8389],\n",
      "          [ 0.4120, -0.0170,  0.0045,  ...,  0.9310,  0.9636,  0.9368],\n",
      "          [ 0.4148, -0.0616, -0.0253,  ...,  0.8389,  0.9368,  0.9944]],\n",
      "\n",
      "         [[ 1.1118,  0.3555,  0.1564,  ...,  0.4578,  0.4700,  0.5077],\n",
      "          [ 0.3555,  1.3358,  0.4521,  ...,  0.1989,  0.1928,  0.1929],\n",
      "          [ 0.1564,  0.4521,  1.4088,  ...,  0.0507,  0.0711,  0.1052],\n",
      "          ...,\n",
      "          [ 0.4578,  0.1989,  0.0507,  ...,  0.7171,  0.7061,  0.6789],\n",
      "          [ 0.4700,  0.1928,  0.0711,  ...,  0.7061,  0.7476,  0.7714],\n",
      "          [ 0.5077,  0.1929,  0.1052,  ...,  0.6789,  0.7714,  0.8595]],\n",
      "\n",
      "         [[ 1.3325,  0.3966,  0.4030,  ...,  0.6364,  0.5887,  0.5500],\n",
      "          [ 0.3966,  1.5136,  0.3060,  ...,  0.2317,  0.2402,  0.2041],\n",
      "          [ 0.4030,  0.3060,  1.3039,  ...,  0.1836,  0.1622,  0.1399],\n",
      "          ...,\n",
      "          [ 0.6364,  0.2317,  0.1836,  ...,  1.2948,  1.2326,  1.0938],\n",
      "          [ 0.5887,  0.2402,  0.1622,  ...,  1.2326,  1.2329,  1.1576],\n",
      "          [ 0.5500,  0.2041,  0.1399,  ...,  1.0938,  1.1576,  1.1671]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "mask.shape torch.Size([140])\n",
      "mask: tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "mask.unsqueeze.shape torch.Size([140, 1])\n",
      "mask.unsqueeze tensor([[ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False]])\n",
      "weights torch.Size([1, 5, 140, 140])\n",
      "weights tensor([[[[ 1.4868e+00,  1.1133e-01,  1.5787e-01,  ...,  9.1784e-01,\n",
      "            1.0648e+00,  1.2089e+00],\n",
      "          [ 1.1133e-01,  1.2679e+00,  3.5328e-01,  ...,  6.2567e-02,\n",
      "            7.9158e-02,  5.6980e-02],\n",
      "          [ 1.5787e-01,  3.5328e-01,  1.1271e+00,  ...,  1.7961e-01,\n",
      "            2.1575e-01,  2.0790e-01],\n",
      "          ...,\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[ 1.5218e+00,  1.4832e-01,  3.1810e-01,  ...,  7.0129e-01,\n",
      "            7.5791e-01,  8.1607e-01],\n",
      "          [ 1.4832e-01,  1.3289e+00,  3.9721e-02,  ...,  1.8291e-01,\n",
      "            1.7599e-01,  1.5609e-01],\n",
      "          [ 3.1810e-01,  3.9721e-02,  8.6644e-01,  ...,  1.7963e-01,\n",
      "            1.4274e-01,  9.1435e-02],\n",
      "          ...,\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[ 9.3965e-01,  2.0824e-01,  2.5750e-02,  ...,  4.0097e-01,\n",
      "            4.1198e-01,  4.1482e-01],\n",
      "          [ 2.0824e-01,  9.7796e-01,  3.0185e-01,  ...,  1.3254e-02,\n",
      "           -1.6950e-02, -6.1626e-02],\n",
      "          [ 2.5750e-02,  3.0185e-01,  1.2403e+00,  ...,  4.5542e-03,\n",
      "            4.5018e-03, -2.5301e-02],\n",
      "          ...,\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[ 1.1118e+00,  3.5551e-01,  1.5643e-01,  ...,  4.5775e-01,\n",
      "            4.7001e-01,  5.0774e-01],\n",
      "          [ 3.5551e-01,  1.3358e+00,  4.5208e-01,  ...,  1.9889e-01,\n",
      "            1.9284e-01,  1.9291e-01],\n",
      "          [ 1.5643e-01,  4.5208e-01,  1.4088e+00,  ...,  5.0690e-02,\n",
      "            7.1067e-02,  1.0520e-01],\n",
      "          ...,\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[ 1.3325e+00,  3.9664e-01,  4.0296e-01,  ...,  6.3635e-01,\n",
      "            5.8867e-01,  5.5000e-01],\n",
      "          [ 3.9664e-01,  1.5136e+00,  3.0597e-01,  ...,  2.3174e-01,\n",
      "            2.4024e-01,  2.0405e-01],\n",
      "          [ 4.0296e-01,  3.0597e-01,  1.3039e+00,  ...,  1.8359e-01,\n",
      "            1.6215e-01,  1.3989e-01],\n",
      "          ...,\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09]]]], grad_fn=<MaskedFillBackward0>)\n",
      "入力のテンソルサイズ： torch.Size([1, 140, 300])\n",
      "出力のテンソルサイズ： torch.Size([1, 140, 300])\n",
      "Attentionのサイズ： torch.Size([1, 5, 140, 140])\n"
     ]
    }
   ],
   "source": [
    "# 動作確認\n",
    "def text_to_ids(text_list, vcb):\n",
    "    result = torch.zeros(140, dtype=torch.long)\n",
    "    result[0] = vcb['<cls>']\n",
    "    for i, word in enumerate(text_list):\n",
    "        if word in vcb:\n",
    "            result[i+1] = vcb[word]\n",
    "        else:\n",
    "            result[i+1] = vcb['<unk>']\n",
    "    for j in range(i+1, 139):\n",
    "        result[j+1] = vcb['<pad>']\n",
    "    return result\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "x = np.load('omomi.npy')\n",
    "x = torch.from_numpy(x.astype(np.float32)).clone()\n",
    "#辞書\n",
    "itos = pickle.load(open('itos.pkl', 'rb'))\n",
    "stoi = pickle.load(open('stoi.pkl', 'rb'))\n",
    "\n",
    "\n",
    "import MeCab\n",
    "\n",
    "mecab = MeCab.Tagger('-Owakati')\n",
    "# バッチサイズ分文章とラベルのセットを取り出す\n",
    "s = 'その棚にある赤いりんごはとてもまずい'\n",
    "result = [tok for tok in mecab.parse(s).split()]\n",
    "print(result)\n",
    "#id列に変換\n",
    "result = text_to_ids(result, stoi)\n",
    "\n",
    "\n",
    "# モデル構築\n",
    "net1 = Embedder(x)\n",
    "net2 = PositionalEncoder(d_model=300, max_seq_len=140)\n",
    "net3 = TransformerBlock(d_model=300, head_num=5)\n",
    "\n",
    "# maskの作成\n",
    "input_pad = 1  # 単語のIDにおいて、'<pad>': 1 なので\n",
    "input_mask = (result != input_pad)\n",
    "print(input_mask)\n",
    "\n",
    "# 入出力\n",
    "x1 = net1(result)  # 単語をベクトルに\n",
    "x2 = net2(x1)  # Positon情報を足し算\n",
    "x3, normlized_weights = net3(x2, input_mask)  # Self-Attentionで特徴量を変換\n",
    "\n",
    "print(\"入力のテンソルサイズ：\", x2.shape)\n",
    "print(\"出力のテンソルサイズ：\", x3.shape)\n",
    "print(\"Attentionのサイズ：\", normlized_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    '''Transformer_Blockの出力を使用し、最後にクラス分類させる'''\n",
    "\n",
    "    def __init__(self, d_model=300, output_dim=5):\n",
    "        super().__init__()\n",
    "\n",
    "        # 全結合層\n",
    "        self.linear = nn.Linear(d_model, output_dim)  \n",
    "\n",
    "        # 重み初期化処理\n",
    "        nn.init.normal_(self.linear.weight, std=0.02)\n",
    "        nn.init.normal_(self.linear.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = x[:, 0, :]  # <cls>の結果を用いる\n",
    "        out = self.linear(x0)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最終的なTransformerモデルのクラス\n",
    "\n",
    "\n",
    "class TransformerEncoderClassification(nn.Module):\n",
    "\n",
    "    def __init__(self, text_embedding_vectors, head_num, dropout=0.1, d_model=300, max_seq_len=140, output_dim=5):\n",
    "        super().__init__()\n",
    "\n",
    "        # モデル構築\n",
    "        self.net1 = Embedder(text_embedding_vectors)\n",
    "        self.net2 = PositionalEncoder(d_model=d_model, max_seq_len=max_seq_len)\n",
    "        self.net3 = nn.Dropout(dropout)\n",
    "        self.net4_1 = TransformerBlock(d_model=d_model, head_num=head_num, dropout=dropout)\n",
    "        self.net4_2 = TransformerBlock(d_model=d_model, head_num=head_num, dropout=dropout)\n",
    "        self.net5 = ClassificationHead(output_dim=output_dim, d_model=d_model)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x1 = self.net1(x)  #Embedding\n",
    "        x2 = self.net2(x1) #PositinalEncoding\n",
    "        x3 = self.net3(x2) #Dropout\n",
    "        x4_1, normlized_weights_1 = self.net4_1(x3, mask) #self-Attention+FFN \n",
    "        x4_2, normlized_weights_2 = self.net4_2(x4_1, mask)  #self-Attention+FFN\n",
    "        x5 = self.net5(x4_2)  #linear\n",
    "        return x5, normlized_weights_1, normlized_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['最近', 'は', '家', 'に', 'いる', 'こと', 'が', '多い', '。']\n",
      "torch.Size([140])\n",
      "weights torch.Size([1, 5, 140, 140])\n",
      "weights tensor([[[[ 1.0387, -0.1192, -0.0029,  ...,  0.4177,  0.6586,  0.5270],\n",
      "          [-0.1192,  1.1612,  0.5589,  ...,  0.0115,  0.0207, -0.0876],\n",
      "          [-0.0029,  0.5589,  1.1645,  ...,  0.1124,  0.0644,  0.0047],\n",
      "          ...,\n",
      "          [ 0.4177,  0.0115,  0.1124,  ...,  1.1141,  1.0060,  0.8966],\n",
      "          [ 0.6586,  0.0207,  0.0644,  ...,  1.0060,  1.1863,  1.0442],\n",
      "          [ 0.5270, -0.0876,  0.0047,  ...,  0.8966,  1.0442,  1.2023]],\n",
      "\n",
      "         [[ 1.2119,  0.1156, -0.0666,  ...,  0.6467,  0.6204,  0.5616],\n",
      "          [ 0.1156,  1.2161,  0.7387,  ...,  0.1356, -0.0184,  0.0475],\n",
      "          [-0.0666,  0.7387,  1.2458,  ...,  0.1305,  0.0195, -0.0093],\n",
      "          ...,\n",
      "          [ 0.6467,  0.1356,  0.1305,  ...,  1.3693,  1.1167,  0.9455],\n",
      "          [ 0.6204, -0.0184,  0.0195,  ...,  1.1167,  1.2246,  0.9958],\n",
      "          [ 0.5616,  0.0475, -0.0093,  ...,  0.9455,  0.9958,  1.1419]],\n",
      "\n",
      "         [[ 1.1534,  0.2304,  0.4741,  ...,  0.4629,  0.5137,  0.4767],\n",
      "          [ 0.2304,  1.3290,  0.5125,  ...,  0.0328,  0.0615, -0.0435],\n",
      "          [ 0.4741,  0.5125,  1.4329,  ...,  0.0604,  0.0623,  0.0737],\n",
      "          ...,\n",
      "          [ 0.4629,  0.0328,  0.0604,  ...,  1.0641,  0.8742,  0.7550],\n",
      "          [ 0.5137,  0.0615,  0.0623,  ...,  0.8742,  1.0090,  0.8457],\n",
      "          [ 0.4767, -0.0435,  0.0737,  ...,  0.7550,  0.8457,  1.0408]],\n",
      "\n",
      "         [[ 1.7189, -0.0249, -0.1093,  ...,  0.8972,  0.9689,  0.7978],\n",
      "          [-0.0249,  1.2058,  0.2262,  ..., -0.1723, -0.2440, -0.2697],\n",
      "          [-0.1093,  0.2262,  1.0029,  ..., -0.1373, -0.1725, -0.0999],\n",
      "          ...,\n",
      "          [ 0.8972, -0.1723, -0.1373,  ...,  1.4695,  1.4599,  1.1068],\n",
      "          [ 0.9689, -0.2440, -0.1725,  ...,  1.4599,  1.6170,  1.1940],\n",
      "          [ 0.7978, -0.2697, -0.0999,  ...,  1.1068,  1.1940,  1.2324]],\n",
      "\n",
      "         [[ 1.4254,  0.1808,  0.1256,  ...,  0.6123,  0.8324,  0.4661],\n",
      "          [ 0.1808,  1.2283,  0.4011,  ...,  0.0364,  0.1072,  0.0991],\n",
      "          [ 0.1256,  0.4011,  1.1270,  ..., -0.2202, -0.2221, -0.1142],\n",
      "          ...,\n",
      "          [ 0.6123,  0.0364, -0.2202,  ...,  1.3922,  1.3414,  0.9840],\n",
      "          [ 0.8324,  0.1072, -0.2221,  ...,  1.3414,  1.6155,  1.1636],\n",
      "          [ 0.4661,  0.0991, -0.1142,  ...,  0.9840,  1.1636,  1.1482]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "mask.shape torch.Size([140])\n",
      "mask: tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "mask.unsqueeze.shape torch.Size([140, 1])\n",
      "mask.unsqueeze tensor([[ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False]])\n",
      "weights torch.Size([1, 5, 140, 140])\n",
      "weights tensor([[[[ 1.0387e+00, -1.1919e-01, -2.9241e-03,  ...,  4.1774e-01,\n",
      "            6.5862e-01,  5.2701e-01],\n",
      "          [-1.1919e-01,  1.1612e+00,  5.5889e-01,  ...,  1.1549e-02,\n",
      "            2.0700e-02, -8.7602e-02],\n",
      "          [-2.9241e-03,  5.5889e-01,  1.1645e+00,  ...,  1.1245e-01,\n",
      "            6.4440e-02,  4.7001e-03],\n",
      "          ...,\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[ 1.2119e+00,  1.1558e-01, -6.6613e-02,  ...,  6.4669e-01,\n",
      "            6.2036e-01,  5.6162e-01],\n",
      "          [ 1.1558e-01,  1.2161e+00,  7.3874e-01,  ...,  1.3562e-01,\n",
      "           -1.8380e-02,  4.7526e-02],\n",
      "          [-6.6613e-02,  7.3874e-01,  1.2458e+00,  ...,  1.3046e-01,\n",
      "            1.9541e-02, -9.3430e-03],\n",
      "          ...,\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[ 1.1534e+00,  2.3037e-01,  4.7414e-01,  ...,  4.6290e-01,\n",
      "            5.1367e-01,  4.7666e-01],\n",
      "          [ 2.3037e-01,  1.3290e+00,  5.1249e-01,  ...,  3.2817e-02,\n",
      "            6.1486e-02, -4.3520e-02],\n",
      "          [ 4.7414e-01,  5.1249e-01,  1.4329e+00,  ...,  6.0422e-02,\n",
      "            6.2323e-02,  7.3684e-02],\n",
      "          ...,\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[ 1.7189e+00, -2.4932e-02, -1.0932e-01,  ...,  8.9724e-01,\n",
      "            9.6893e-01,  7.9778e-01],\n",
      "          [-2.4932e-02,  1.2058e+00,  2.2621e-01,  ..., -1.7228e-01,\n",
      "           -2.4404e-01, -2.6971e-01],\n",
      "          [-1.0932e-01,  2.2621e-01,  1.0029e+00,  ..., -1.3726e-01,\n",
      "           -1.7246e-01, -9.9936e-02],\n",
      "          ...,\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[ 1.4254e+00,  1.8083e-01,  1.2563e-01,  ...,  6.1229e-01,\n",
      "            8.3237e-01,  4.6614e-01],\n",
      "          [ 1.8083e-01,  1.2283e+00,  4.0108e-01,  ...,  3.6424e-02,\n",
      "            1.0720e-01,  9.9106e-02],\n",
      "          [ 1.2563e-01,  4.0108e-01,  1.1270e+00,  ..., -2.2023e-01,\n",
      "           -2.2215e-01, -1.1418e-01],\n",
      "          ...,\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09]]]], grad_fn=<MaskedFillBackward0>)\n",
      "weights torch.Size([1, 5, 140, 140])\n",
      "weights tensor([[[[ 1.0598,  0.4437,  0.2184,  ...,  0.5000,  0.3666,  0.4991],\n",
      "          [ 0.4437,  1.2495,  0.4315,  ...,  0.2095,  0.1992,  0.1232],\n",
      "          [ 0.2184,  0.4315,  0.7125,  ..., -0.0555, -0.0828, -0.1606],\n",
      "          ...,\n",
      "          [ 0.5000,  0.2095, -0.0555,  ...,  1.0723,  0.8968,  0.8636],\n",
      "          [ 0.3666,  0.1992, -0.0828,  ...,  0.8968,  0.9434,  0.8556],\n",
      "          [ 0.4991,  0.1232, -0.1606,  ...,  0.8636,  0.8556,  1.1442]],\n",
      "\n",
      "         [[ 1.3077,  0.2446,  0.1071,  ...,  0.5711,  0.5608,  0.6720],\n",
      "          [ 0.2446,  1.2188,  0.2983,  ..., -0.0097, -0.0024, -0.0277],\n",
      "          [ 0.1071,  0.2983,  0.8095,  ...,  0.1087,  0.1294,  0.1894],\n",
      "          ...,\n",
      "          [ 0.5711, -0.0097,  0.1087,  ...,  1.1613,  0.8974,  0.8546],\n",
      "          [ 0.5608, -0.0024,  0.1294,  ...,  0.8974,  1.0552,  1.0146],\n",
      "          [ 0.6720, -0.0277,  0.1894,  ...,  0.8546,  1.0146,  1.3526]],\n",
      "\n",
      "         [[ 1.1091,  0.3884, -0.0985,  ...,  0.4113,  0.3140,  0.2377],\n",
      "          [ 0.3884,  1.2583,  0.2551,  ...,  0.0756, -0.0286, -0.0619],\n",
      "          [-0.0985,  0.2551,  1.0745,  ..., -0.0562, -0.1130,  0.0067],\n",
      "          ...,\n",
      "          [ 0.4113,  0.0756, -0.0562,  ...,  1.1004,  0.8538,  0.6928],\n",
      "          [ 0.3140, -0.0286, -0.1130,  ...,  0.8538,  0.9255,  0.6584],\n",
      "          [ 0.2377, -0.0619,  0.0067,  ...,  0.6928,  0.6584,  0.9753]],\n",
      "\n",
      "         [[ 1.1579, -0.0373,  0.1674,  ...,  0.6184,  0.8209,  0.7741],\n",
      "          [-0.0373,  1.2294,  0.5196,  ..., -0.0755, -0.1155, -0.0639],\n",
      "          [ 0.1674,  0.5196,  1.0096,  ..., -0.0242,  0.0770, -0.0441],\n",
      "          ...,\n",
      "          [ 0.6184, -0.0755, -0.0242,  ...,  1.2791,  1.2440,  1.0543],\n",
      "          [ 0.8209, -0.1155,  0.0770,  ...,  1.2440,  1.5506,  1.2993],\n",
      "          [ 0.7741, -0.0639, -0.0441,  ...,  1.0543,  1.2993,  1.4768]],\n",
      "\n",
      "         [[ 1.1974,  0.2274,  0.1880,  ...,  0.6487,  0.7913,  0.8289],\n",
      "          [ 0.2274,  1.1085,  0.2817,  ...,  0.1894,  0.3152,  0.2933],\n",
      "          [ 0.1880,  0.2817,  0.9682,  ...,  0.1867,  0.2122,  0.3673],\n",
      "          ...,\n",
      "          [ 0.6487,  0.1894,  0.1867,  ...,  1.2105,  0.9531,  0.9021],\n",
      "          [ 0.7913,  0.3152,  0.2122,  ...,  0.9531,  1.1259,  1.0148],\n",
      "          [ 0.8289,  0.2933,  0.3673,  ...,  0.9021,  1.0148,  1.4022]]]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "mask.shape torch.Size([140])\n",
      "mask: tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False])\n",
      "mask.unsqueeze.shape torch.Size([140, 1])\n",
      "mask.unsqueeze tensor([[ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False],\n",
      "        [False]])\n",
      "weights torch.Size([1, 5, 140, 140])\n",
      "weights tensor([[[[ 1.0598e+00,  4.4369e-01,  2.1840e-01,  ...,  5.0001e-01,\n",
      "            3.6661e-01,  4.9909e-01],\n",
      "          [ 4.4369e-01,  1.2495e+00,  4.3150e-01,  ...,  2.0949e-01,\n",
      "            1.9918e-01,  1.2319e-01],\n",
      "          [ 2.1840e-01,  4.3150e-01,  7.1253e-01,  ..., -5.5532e-02,\n",
      "           -8.2821e-02, -1.6062e-01],\n",
      "          ...,\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[ 1.3077e+00,  2.4456e-01,  1.0713e-01,  ...,  5.7111e-01,\n",
      "            5.6078e-01,  6.7198e-01],\n",
      "          [ 2.4456e-01,  1.2188e+00,  2.9829e-01,  ..., -9.7425e-03,\n",
      "           -2.3687e-03, -2.7658e-02],\n",
      "          [ 1.0713e-01,  2.9829e-01,  8.0946e-01,  ...,  1.0872e-01,\n",
      "            1.2944e-01,  1.8941e-01],\n",
      "          ...,\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[ 1.1091e+00,  3.8838e-01, -9.8459e-02,  ...,  4.1132e-01,\n",
      "            3.1401e-01,  2.3767e-01],\n",
      "          [ 3.8838e-01,  1.2583e+00,  2.5512e-01,  ...,  7.5574e-02,\n",
      "           -2.8576e-02, -6.1862e-02],\n",
      "          [-9.8459e-02,  2.5512e-01,  1.0745e+00,  ..., -5.6195e-02,\n",
      "           -1.1295e-01,  6.6850e-03],\n",
      "          ...,\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[ 1.1579e+00, -3.7316e-02,  1.6739e-01,  ...,  6.1841e-01,\n",
      "            8.2085e-01,  7.7407e-01],\n",
      "          [-3.7316e-02,  1.2294e+00,  5.1960e-01,  ..., -7.5488e-02,\n",
      "           -1.1554e-01, -6.3903e-02],\n",
      "          [ 1.6739e-01,  5.1960e-01,  1.0096e+00,  ..., -2.4221e-02,\n",
      "            7.6969e-02, -4.4150e-02],\n",
      "          ...,\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "         [[ 1.1974e+00,  2.2739e-01,  1.8800e-01,  ...,  6.4873e-01,\n",
      "            7.9126e-01,  8.2886e-01],\n",
      "          [ 2.2739e-01,  1.1085e+00,  2.8167e-01,  ...,  1.8942e-01,\n",
      "            3.1519e-01,  2.9333e-01],\n",
      "          [ 1.8800e-01,  2.8167e-01,  9.6817e-01,  ...,  1.8672e-01,\n",
      "            2.1217e-01,  3.6726e-01],\n",
      "          ...,\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09],\n",
      "          [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
      "           -1.0000e+09, -1.0000e+09]]]], grad_fn=<MaskedFillBackward0>)\n",
      "出力のテンソルサイズ： torch.Size([1, 5])\n",
      "出力テンソルのsigmoid： tensor([[0.0310, 0.5775, 0.1163, 0.0386, 0.2366]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 動作確認\n",
    "mecab = MeCab.Tagger('-Owakati')\n",
    "# バッチサイズ分文章とラベルのセットを取り出す\n",
    "s = '最近は家にいることが多い。'\n",
    "result = [tok for tok in mecab.parse(s).split()]\n",
    "print(result)\n",
    "#id列に変換\n",
    "result = text_to_ids(result, stoi)\n",
    "print(result.shape)\n",
    "\n",
    "# モデル構築\n",
    "net = TransformerEncoderClassification(\n",
    "    text_embedding_vectors=x, head_num=5, dropout=0.1, d_model=300, max_seq_len=140, output_dim=5)\n",
    "\n",
    "# 入出力\n",
    "\n",
    "input_mask = (result != input_pad)\n",
    "out, normlized_weights_1, normlized_weights_2 = net(result, input_mask)\n",
    "\n",
    "print(\"出力のテンソルサイズ：\", out.shape)\n",
    "print(\"出力テンソルのsigmoid：\", F.softmax(out, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
